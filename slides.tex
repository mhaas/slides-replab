\documentclass[12pt,a4paper]{beamer}
\usepackage[utf8x]{inputenc}
\usepackage{ucs}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\author{Michael Haas, Tilman Wittl}
\title{Overview of RepLab 2012: Evaluating Online
Reputation Management Systems
}
\date{18. Januar 2013}
\begin{document}
\begin{frame}
\maketitle
\end{frame}

\begin{frame}{Overview}
\begin{itemize}
\item Introduction
\item RepLab 2012
\begin{itemize}
\item Tasks
\item Data
\item Evaluation
\item Systems
\item Overall Results
\end{itemize}
\item Our Approach
\end{itemize}
\end{frame}

\begin{frame}{Introduction}
\begin{itemize}
\item For companies and public figures a good reputation is an important component in public relations.
\item Due to the availability of online media traditional advertising is not the only way to build and keep an image.
\item Online media reach more people and nearly everyone can participate.
\item Opinions can be spread very fast to a large audience.
\item Therefore there is a need to analyse online media automatically.
\end{itemize}
\end{frame}
\begin{frame}{Online Reputation Management (ORM)}
\begin{itemize}
\item Online Reputation Management (ORM) subsumes all methods to analyze online media in order to maintain the reputation of a company or public figure.
\item Obviously natural language processing is a key player in ORM.
\item ORM is very related to opinion mining, but has a focus on companies and public figures not mainly on products.
\end{itemize}
\end{frame}

\begin{frame}{RepLab 2012}
\begin{itemize}
\item Workshop on Online Reputation Management
\item Twitter was used as resource
\item Consists of two tasks -- \textit{Monitoring} and \textit{Profiling}
\end{itemize}
\end{frame}

\begin{frame}{Monitoring}
\begin{itemize}
\item Continuous analysis of online media
\item Recognize issues that might affect a company/public figure
\item Systems had to process a stream of tweets
\item In this task two subtasks had to be solved
\begin{itemize}
\item Cluster tweets thematically
\item Assign priority score to clusters
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Profiling}
\begin{itemize}
\item In contrast to monitoring task no continuous analysis
\item Systems had to annotate tweets with the following information:
\begin{itemize}
\item Relevance/filtering (e.g. Apple -- fruit or company)
\item Polarity (positive -- neutral -- negative)
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Data}
\begin{itemize}
\item Trial data:
\begin{itemize}
\item Ca. 30,000 tweets crawled for each of six companies (Apple, Lufthansa, Alcatel, Armani, Marriott, Barclays)
\item In English and Spanish
\item With metainformation (link to Wikipedia, link to website, ...)
\item 300 tweets for each company are annotated by reputation experts
\end{itemize}
\item Test data:
\begin{itemize}
\item At least 50,000 tweets per 31 companies
\item In English and Spanish
\item Companies differ from the trial data
\end{itemize}
\item Test data is much larger and different than trial data
\item Machine learning is not possible
\end{itemize}
\end{frame}


\begin{frame}{Evaluation}
\begin{itemize}
\item Evaluation is difficult because the tasks consist of subtasks
\item Measures:
\begin{itemize}
\item Reliability: \textit{[...] probability of finding the document relationships in the gold standard when they appear in the output [...]} \cite{amigo}
\item Sensitivity: \textit{[...] probability of finding the gold standard relationships in the system output [...]} \cite{amigo}
\item Accuracy
\item F(R,S): harmonic mean between Reliability and Sensitivity
\end{itemize}
\end{itemize}
\end{frame}



%% Michael
\begin{frame}{Systems -- OXYME -- Approach}
Oxyme, Amsterdam \cite{oxyme}
\begin{itemize}
\item Machine Learning
\item Naive Bayes
\item SVM
\item Decision Trees (J48)
\item Features: combination of related work
\end{itemize}

\end{frame}

\begin{frame}{Systems -- OXYME -- Features -- Query dependent features}
Features depending on query, not tweet content!
\begin{itemize}
\item Wikipedia disambiguation page?
\item Common first or last name?
\item Dictionary entry?
\item Query term identical to entity name
\item Amount of negative feedback terms
\item Query difficulty: combination of all features
\end{itemize}

\end{frame}


\begin{frame}{Systems -- OXYME -- Features -- Relevancy Features}
\begin{itemize}
\item Language modeling approach w/ manual relevance feedback
\item For each query in background corpus: create two wordclouds from most frequently used terms
  \begin{itemize}
  \item One cloud from tweets containing term as separate word
  \item One cloud from tweets not containing term as separate word
  \end{itemize}
\item Nivea
  \begin{itemize}
  \item Negative feedback terms: Nivea, "Don't mess with my men", @nivea\_mariee\_
  \item Positive feedback terms: body, cream, care
  \end{itemize}
\end{itemize}

\end{frame}


\begin{frame}{Systems -- OXYME -- Features -- Relevancy Score}
\begin{math} logP(R|D,Q) = logP(R|D) + \sum_{ t \in Q_{pos} } {P(t|Q)*logP(t|D) - P(t|Q)*logP(t|C)} - 
    \sum_{ t \in Q_{neg} } { P(t|Q)logP(t|D) - P(t|Q)logP(t|C) }
\end{math}

\end{frame}

\begin{frame}{Systems -- OXYME -- Features -- Tweet Features}
\begin{itemize}
\item Tweet length
\item Tweet contains link
\item Tweet contains link to domain
\item Tweet starts with @username
\item Tweet is retweet
\item Tweet is question
\end{itemize}

\end{frame}


\begin{frame}{Systems -- OXYME -- Features -- Tweet Features}
\begin{itemize}
\item SentiStrength, derived from MySpace comments (Thelwall, 2010)
\item Generates sentiment scores based on word lists + associated weights
\item Rated training tweets used to add words to dictionary and optimize weights
\end{itemize}

\end{frame}

\begin{frame}{Systems -- OXYME -- Results -- Filtering Subtask}
Filtering Subtask
\begin{itemize}
\item OXY\_2: Naive Bayes, both languages
  \begin{itemize}
  \item Accuracy 0.81
  \item F(R,S): 0.197
  \end{itemize}
\item OXY\_1: SVM, both languages
  \begin{itemize}
  \item Accuracy: 0.798
  \item F(R,S): 0.139
  \end{itemize}
\item Baseline: all relevant
  \begin{itemize}
  \item Accuracy: 0.71
  \item F(R,S): 0
  \end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Systems -- OXYME -- Results -- Polarity Subtask}
Polarity Subtask
\begin{itemize}
\item OXY\_3: J48, all features, both languages
  \begin{itemize}
  \item Accuracy: 0.38
  \item F(R,S): 0.19
  \end{itemize}
\item OXY\_4: J48, SentiStrength features, both languages
  \begin{itemize}
   \item Accuracy: 0.35
   \item F(R,S): 0.26
  \end{itemize}
\item Baseline: all positive
  \begin{itemize}
  \item Accuracy: 0.44
  \item Best run in workshop: Accuracy: 0.49
  \item F(R,S): 0
  \item Best run in workshop: 0.40
  \end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Systems -- OXYME -- Results -- Overall}
Overall Results
\begin{itemize}
 \item OXY\_2: Naive Bayes classifier, all features, both languages
  \begin{itemize}
  \item Accuracy: 0.412
  \end{itemize}
\item OXY\_1: SVM, all features, both languages
  \begin{itemize}
  \item Accuracy: 0.387
  \end{itemize}
\item Baseline: all relevant and positive
  \begin{itemize}
  \item Accuracy: 0.27
  \end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Systems -- GAVAGAI -- Approach}
\begin{itemize}
\item Commercial System: Ethersource by Gavagai \cite{gavagai}
\item Distributional semantics represented in a semantic space
\end{itemize}

\end{frame}


\begin{frame}{Systems -- GAVAGAI -- Filtering}
\begin{itemize}
\item Company defined as target
\item Target defined through name
\item Additional/blocking terms from semantic space model
  \begin{itemize}
  \item 'blackberry' - '\#rim'
  \end{itemize}
\end{itemize}

\end{frame}

\begin{frame}{Systems -- GAVAGAI -- Polarity}
\begin{itemize}
\item Semantic poles: list of positive and negative terms
\item "Consumer Satisfaction" pole for English and Spanish
\item terms selected by editors, semi-automatically augmented
\item If target identifier fires, polarity score is computed
\end{itemize}

\end{frame}

\begin{frame}{Systems -- GAVAGAI -- Results}
Filtering
\begin{itemize}
\item Accuracy: 0.77
\item F(R,S): 0.22
\end{itemize}
Polarity
\begin{itemize}
\item Accuracy: 0.38
\item F(R,S): 0.28
\end{itemize}
Overall
\begin{itemize}
\item Accuracy: 0.40
\end{itemize}

\end{frame}



%% End Michael



\begin{frame}{Systems -- UNED -- Filtering \cite{uned}}
\begin{itemize}
\item sentence splitting and tokenization with GATE
\item stop words and symbols were removed
\item filtering is done by four rules
\end{itemize}
\begin{itemize}
\item Rules
\begin{enumerate}
\item text contains (full) entity name (e.g. Apple Inc.)
\item text contains part of entity name (e.g. Mercedes for Mercedes--Benz)
\item text contains entity name without whitespace (e.g. \#MercedesBenz)
\item Wikipedia entry and website contains words of text
\end{enumerate}
\end{itemize}
\end{frame}








\begin{frame}{Systems -- UNED -- Polarity}
\begin{itemize}
\item To determine the polarity of a tweet the authors extended an existing approach
\end{itemize}
\begin{enumerate}
\item Pre--processing: POS Tagging (GATE, FreeLing) and Concept Identification (parsing and wsd to WordNet--Synsets)
\item Emotion Identification: Mapping from WordNet--Synsets to SentiSense lexicon
\item Post--processing: Negations and intensifiers were marked by using a list of common used phrases. The scope was determined by the parse trees.
\item Classification: Generated informations were packed in a so called Vector of Emotional Intensities (VEI) which represents the information from the SentiSense lexicon.
\item Classification algorithm: variant of Heterogeneity Based Ranking (uses a set of classifiers)
\end{enumerate}

\end{frame}

\begin{frame}{Systems -- UNED -- Results}
Filtering
\begin{itemize}
\item Accuracy: 0.71
\item F(R,S): 0.13
\end{itemize}
Polarity
\begin{itemize}
\item Accuracy: 0.49 
\item F(R,S): 0.31
\end{itemize}
Overall
\begin{itemize}
\item Accuracy: 0.39
\end{itemize}

\end{frame}


\begin{frame}{Overall -- Results}
\begin{itemize}
\item Monitoring:
\begin{itemize}
\item NO system did beat the baseline!
\end{itemize}
\item Profiling (best 3 systems):
\begin{itemize}
\item OXYME (0.41)
\item Gavagai (0.40)
\item UNED (0.39)
\item Baseline: all relevant and positive (0.27)
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Our Approach -- Overview}
\begin{itemize}
\item RepLab 2012 is over (and too hard)
\item RepLab 2013 too late - in September
\item Let's build a web service: sentiment analysis for German tweets!
  \begin{itemize}
  \item Get sense of sentiment about given \#topic
  \item --> Polarity for reputation
  \end{itemize}
\end{itemize}
\end{frame}



\begin{frame}{Our Approach -- Related Work}
Not many people working on German data.
\begin{itemize}
\item \textit{Predicting Elections with Twitter} \cite{elections}
  \begin{itemize}
  \item Translates German tweets with Google Translate :(
  \end{itemize}
\item \textit{Language-Independent Twitter Sentiment Analysis} \cite{narr}
  \begin{itemize}
  \item Semi-supervised learning approach
  \item Tweet labels generated from emoticons
  \item More data is better data
  \item Manually annotated test set available (!)
  \end{itemize}
\end{itemize}
\end{frame}


\begin{frame}{Our Approach -- Details}
\begin{itemize}
\item Grab German tweets from Twitter streaming API
  \begin{itemize}
  \item 1000 per hour, 1000 out of 18500 ":)", 155 ":("
  \end{itemize}
\item Generate labeled training data from tweets containing emoticons \cite{goodbadomg} \cite{emotiontokens} \cite{sentimentanalysistwitter}
\item Optional: generate more labeled training data from manually labeled hash tags \cite{goodbadomg}  
\item Extract features based on related work
\item Train machine learning classifier
\item Evaluate classifier on test set -- \cite{narr}
\item Build web service interfacing with classifier
\end{itemize}
\end{frame}




\begin{frame}[allowframebreaks]{References}
\begin{thebibliography}{-}
\bibitem{amigo} Amigo, E., Gonzalo, J., Verdejo, F.: Reliability and sensitivity: Generic evaluation measures for document organization tasks. Technical report, UNED (2012)
\bibitem{replab} Amigo et al: Overview of RepLab 2012: Evaluation Online Reputation Management Systems. 
\bibitem{oxyme} Kaptein, R.: Learning to Anlyze Relevancy and Polarity of Tweets. CLEF Online Working Notes/Labs/Workshop, (2012)
\bibitem{gavagai} Karlgren et al: Profiling Reputation of Corporate Entities in Semantic Space. CLEF Online Working Notes/Labs/Workshop, (2012)
\bibitem{elections} Tumasjan et al.: Predicting Elections with Twitter: What 140 Characters reveal about Political Sentiment. Proceedings of the Fourth International AAAI conference on Weblogs and Social Media (2010)
\bibitem{narr} Narr, S., Hülfenhaus, M., Albayrak, S.: Language-Independent Twitter Sentiment Analysis. Proceedings KDML-2012
\bibitem{goodbadomg} Kouloumpis E., Wilson T., Moore J.: Twitter Sentiment Analysis: The Good the Bad and the OMG!.  Proceedings of the Fifth International AAAI conference on Weblogs and Social Media (2011)
\bibitem{emotiontokens} Cui et al.: Emotion tokens: Bridging the Gap Among Multilingual Twitter Sentiment Analysis, Proceedings of 7th Asia Information Retrieval Societies Conference (2011)
\bibitem{sentimentanalysistwitter} Agarwal et al.: Sentiment Analysis of Twitter Data. Proceeding LSM '11 Proceedings of the Workshop on Languages in Social Media (2011)
\bibitem{uned} Carrillo-de-Albornoz, J., Chugur, I., Amigó, E.: Using an Emotion-based Model and Sentiment Analysis Techniques to Classify Polarity for Reputation. CLEF 2012 Lab Reports (2012)
\end{thebibliography}
\end{frame}
\end{document}
